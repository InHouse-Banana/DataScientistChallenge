{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\\\n",
    "                            , recall_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.- Setting global parameters and importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9711 entries, 0 to 9710\n",
      "Data columns (total 17 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   female                      9711 non-null   float64\n",
      " 1   male                        9711 non-null   float64\n",
      " 2   graduate_school             9711 non-null   float64\n",
      " 3   high_school                 9711 non-null   float64\n",
      " 4   university                  9711 non-null   float64\n",
      " 5   unknown                     9711 non-null   float64\n",
      " 6   married                     9711 non-null   float64\n",
      " 7   single                      9711 non-null   float64\n",
      " 8   limit_bal                   9711 non-null   float64\n",
      " 9   age                         9711 non-null   int64  \n",
      " 10  bal_amt1                    9711 non-null   float64\n",
      " 11  bal_amt2                    9711 non-null   float64\n",
      " 12  bal_amt3                    9711 non-null   float64\n",
      " 13  bal_amt4                    9711 non-null   float64\n",
      " 14  bal_amt5                    9711 non-null   float64\n",
      " 15  bal_amt6                    9711 non-null   float64\n",
      " 16  default.payment.next.month  9711 non-null   int64  \n",
      "dtypes: float64(15), int64(2)\n",
      "memory usage: 1.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>graduate_school</th>\n",
       "      <th>high_school</th>\n",
       "      <th>university</th>\n",
       "      <th>unknown</th>\n",
       "      <th>married</th>\n",
       "      <th>single</th>\n",
       "      <th>limit_bal</th>\n",
       "      <th>age</th>\n",
       "      <th>bal_amt1</th>\n",
       "      <th>bal_amt2</th>\n",
       "      <th>bal_amt3</th>\n",
       "      <th>bal_amt4</th>\n",
       "      <th>bal_amt5</th>\n",
       "      <th>bal_amt6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>23</td>\n",
       "      <td>48463.0</td>\n",
       "      <td>47321.0</td>\n",
       "      <td>45791.0</td>\n",
       "      <td>39230.0</td>\n",
       "      <td>26797.0</td>\n",
       "      <td>27364.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>25</td>\n",
       "      <td>7025.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>5911.0</td>\n",
       "      <td>-4510.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>52</td>\n",
       "      <td>86248.0</td>\n",
       "      <td>88033.0</td>\n",
       "      <td>89852.0</td>\n",
       "      <td>90364.0</td>\n",
       "      <td>94431.0</td>\n",
       "      <td>97706.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>280000.0</td>\n",
       "      <td>26</td>\n",
       "      <td>24189.0</td>\n",
       "      <td>25252.0</td>\n",
       "      <td>26311.0</td>\n",
       "      <td>27638.0</td>\n",
       "      <td>28352.0</td>\n",
       "      <td>29217.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  male  graduate_school  high_school  university  unknown  married  \\\n",
       "0     1.0   0.0              0.0          1.0         0.0      0.0      0.0   \n",
       "1     0.0   1.0              0.0          1.0         0.0      0.0      0.0   \n",
       "2     0.0   1.0              0.0          1.0         0.0      0.0      1.0   \n",
       "3     1.0   0.0              0.0          0.0         1.0      0.0      0.0   \n",
       "4     1.0   0.0              1.0          0.0         0.0      0.0      1.0   \n",
       "\n",
       "   single  limit_bal  age  bal_amt1  bal_amt2  bal_amt3  bal_amt4  bal_amt5  \\\n",
       "0     1.0    50000.0   23   48463.0   47321.0   45791.0   39230.0   26797.0   \n",
       "1     1.0    10000.0   25    7025.0     141.0    1239.0    5911.0   -4510.0   \n",
       "2     0.0   150000.0   52   86248.0   88033.0   89852.0   90364.0   94431.0   \n",
       "3     1.0   280000.0   26   24189.0   25252.0   26311.0   27638.0   28352.0   \n",
       "4     0.0   360000.0   41       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   bal_amt6  default.payment.next.month  \n",
       "0   27364.0                           0  \n",
       "1   10000.0                           0  \n",
       "2   97706.0                           1  \n",
       "3   29217.0                           0  \n",
       "4       0.0                           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"data_artifacts/training_features_targets.csv\")\n",
    "display(train_data.info())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.drop( columns = train_data.columns[-1:])\n",
    "y = train_data.drop( columns = train_data.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Split\n",
    "I split the dataset into train/test - 80/20. It would be useful to have a validation dataset, but the dataset small and with some categories with 1 element that I do not have a test validation. This would be useful to have in a production environment. The training is carried out using cross-validation which creates a val dataset for the purpose of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = train_test_split(x, y\\\n",
    "                                                    , test_size = 0.2, random_state= 42\\\n",
    "                                                    , stratify  = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_majority.shape (6014, 16) df_minority.shape (1754, 16)\n",
      "df_majority.shape: (6014, 16) df_minority_upsampled.shape: (6014, 16)\n",
      "X_train_upsampled.shape: (12028, 16)\n",
      "Y_train_upsampled.shape: (12028, 1)\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "index_majority = Y_train[Y_train['default.payment.next.month'] == 0].index.values\n",
    "index_minority = Y_train[Y_train['default.payment.next.month'] == 1].index.values\n",
    "\n",
    "df_majority = X_train.loc[index_majority,:]\n",
    "df_minority = X_train.loc[index_minority,:]\n",
    "\n",
    "print('df_majority.shape', df_majority.shape, 'df_minority.shape', df_minority.shape)\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority \n",
    "                                 , replace      = True     # sample with replacement\n",
    "                                 , n_samples    = df_majority.shape[0]    # to match majority class\n",
    "                                 , random_state = 42) # reproducible results\n",
    "\n",
    "print('df_majority.shape:', df_majority.shape, 'df_minority_upsampled.shape:', df_minority_upsampled.shape)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "X_train_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "Y_train_upsampled = Y_train.loc[X_train_upsampled.index,:]\n",
    "\n",
    "X_train_upsampled = X_train_upsampled.reset_index(drop=True)\n",
    "Y_train_upsampled = Y_train_upsampled.reset_index(drop=True)\n",
    " \n",
    "# Display new class counts\n",
    "print('X_train_upsampled.shape:', X_train_upsampled.shape)\n",
    "print('Y_train_upsampled.shape:', Y_train_upsampled.shape)\n",
    "#training_data_upsampled['default.payment.next.month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('data_artifacts/X_train.csv', index = False)\n",
    "X_train_upsampled.to_csv('data_artifacts/X_train_upsampled.csv', index = False)\n",
    "X_test.to_csv('data_artifacts/X_test.csv', index = False)\n",
    "Y_train.to_csv('data_artifacts/Y_train.csv', index = False)\n",
    "Y_train_upsampled.to_csv('data_artifacts/Y_train_upsampled.csv', index = False)\n",
    "Y_test.to_csv('data_artifacts/Y_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I don't normalise the data for the XGBoost model, it could have been useful if I were comparing different models some of which might require data scaling (linear regression).\n",
    "https://datascience.stackexchange.com/questions/60950/is-it-necessary-to-normalise-data-for-xgboost#:~:text=2%20Answers&text=Your%20rationale%20is%20indeed%20correct,normalization%20for%20the%20inputs%20either\n",
    "\n",
    "https://github.com/dmlc/xgboost/issues/357\n",
    "\n",
    "Also, XGBoost takes care of multicolinearity by default, which is another useful feature of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 XGBoost classifier model baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = ['logloss', 'auc'] #'merror',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.65809\tvalidation_0-auc:0.70994\tvalidation_1-logloss:0.67519\tvalidation_1-auc:0.61035\n",
      "[1]\tvalidation_0-logloss:0.63597\tvalidation_0-auc:0.72785\tvalidation_1-logloss:0.66446\tvalidation_1-auc:0.62625\n",
      "[2]\tvalidation_0-logloss:0.61939\tvalidation_0-auc:0.74872\tvalidation_1-logloss:0.66097\tvalidation_1-auc:0.62184\n",
      "[3]\tvalidation_0-logloss:0.60588\tvalidation_0-auc:0.76061\tvalidation_1-logloss:0.65433\tvalidation_1-auc:0.63487\n",
      "[4]\tvalidation_0-logloss:0.59586\tvalidation_0-auc:0.76859\tvalidation_1-logloss:0.64966\tvalidation_1-auc:0.64211\n",
      "[5]\tvalidation_0-logloss:0.58442\tvalidation_0-auc:0.78069\tvalidation_1-logloss:0.64673\tvalidation_1-auc:0.64377\n",
      "[6]\tvalidation_0-logloss:0.57660\tvalidation_0-auc:0.78997\tvalidation_1-logloss:0.64422\tvalidation_1-auc:0.64495\n",
      "[7]\tvalidation_0-logloss:0.57241\tvalidation_0-auc:0.79421\tvalidation_1-logloss:0.64311\tvalidation_1-auc:0.64700\n",
      "[8]\tvalidation_0-logloss:0.56034\tvalidation_0-auc:0.80695\tvalidation_1-logloss:0.63939\tvalidation_1-auc:0.64825\n",
      "[9]\tvalidation_0-logloss:0.55621\tvalidation_0-auc:0.81044\tvalidation_1-logloss:0.63850\tvalidation_1-auc:0.64854\n",
      "[10]\tvalidation_0-logloss:0.54854\tvalidation_0-auc:0.81705\tvalidation_1-logloss:0.63721\tvalidation_1-auc:0.65073\n",
      "[11]\tvalidation_0-logloss:0.54591\tvalidation_0-auc:0.81959\tvalidation_1-logloss:0.63607\tvalidation_1-auc:0.65148\n",
      "[12]\tvalidation_0-logloss:0.54258\tvalidation_0-auc:0.82277\tvalidation_1-logloss:0.63554\tvalidation_1-auc:0.65034\n",
      "[13]\tvalidation_0-logloss:0.54000\tvalidation_0-auc:0.82530\tvalidation_1-logloss:0.63499\tvalidation_1-auc:0.65019\n",
      "[14]\tvalidation_0-logloss:0.53800\tvalidation_0-auc:0.82664\tvalidation_1-logloss:0.63462\tvalidation_1-auc:0.65058\n",
      "[15]\tvalidation_0-logloss:0.53422\tvalidation_0-auc:0.83036\tvalidation_1-logloss:0.63445\tvalidation_1-auc:0.64894\n",
      "[16]\tvalidation_0-logloss:0.52771\tvalidation_0-auc:0.83644\tvalidation_1-logloss:0.63301\tvalidation_1-auc:0.64870\n",
      "[17]\tvalidation_0-logloss:0.51922\tvalidation_0-auc:0.84564\tvalidation_1-logloss:0.63321\tvalidation_1-auc:0.64399\n",
      "[18]\tvalidation_0-logloss:0.51717\tvalidation_0-auc:0.84806\tvalidation_1-logloss:0.63236\tvalidation_1-auc:0.64385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\tvalidation_0-logloss:0.51358\tvalidation_0-auc:0.85188\tvalidation_1-logloss:0.63232\tvalidation_1-auc:0.64296\n",
      "[20]\tvalidation_0-logloss:0.51218\tvalidation_0-auc:0.85276\tvalidation_1-logloss:0.63111\tvalidation_1-auc:0.64423\n",
      "[21]\tvalidation_0-logloss:0.50869\tvalidation_0-auc:0.85647\tvalidation_1-logloss:0.63159\tvalidation_1-auc:0.64334\n",
      "[22]\tvalidation_0-logloss:0.49852\tvalidation_0-auc:0.86780\tvalidation_1-logloss:0.62891\tvalidation_1-auc:0.64346\n",
      "[23]\tvalidation_0-logloss:0.49778\tvalidation_0-auc:0.86858\tvalidation_1-logloss:0.62887\tvalidation_1-auc:0.64293\n",
      "[24]\tvalidation_0-logloss:0.49551\tvalidation_0-auc:0.87041\tvalidation_1-logloss:0.62842\tvalidation_1-auc:0.64310\n",
      "[25]\tvalidation_0-logloss:0.48889\tvalidation_0-auc:0.87736\tvalidation_1-logloss:0.62684\tvalidation_1-auc:0.64144\n",
      "[26]\tvalidation_0-logloss:0.48657\tvalidation_0-auc:0.87909\tvalidation_1-logloss:0.62673\tvalidation_1-auc:0.64165\n",
      "[27]\tvalidation_0-logloss:0.48101\tvalidation_0-auc:0.88466\tvalidation_1-logloss:0.62445\tvalidation_1-auc:0.64252\n",
      "[28]\tvalidation_0-logloss:0.47477\tvalidation_0-auc:0.88967\tvalidation_1-logloss:0.62304\tvalidation_1-auc:0.64229\n",
      "[29]\tvalidation_0-logloss:0.47152\tvalidation_0-auc:0.89298\tvalidation_1-logloss:0.62263\tvalidation_1-auc:0.64210\n",
      "[30]\tvalidation_0-logloss:0.46913\tvalidation_0-auc:0.89470\tvalidation_1-logloss:0.62154\tvalidation_1-auc:0.64386\n",
      "[31]\tvalidation_0-logloss:0.46769\tvalidation_0-auc:0.89556\tvalidation_1-logloss:0.62170\tvalidation_1-auc:0.64368\n",
      "[32]\tvalidation_0-logloss:0.46610\tvalidation_0-auc:0.89657\tvalidation_1-logloss:0.62157\tvalidation_1-auc:0.64372\n",
      "[33]\tvalidation_0-logloss:0.46345\tvalidation_0-auc:0.89877\tvalidation_1-logloss:0.62080\tvalidation_1-auc:0.64354\n",
      "[34]\tvalidation_0-logloss:0.46153\tvalidation_0-auc:0.89990\tvalidation_1-logloss:0.62115\tvalidation_1-auc:0.64289\n",
      "[35]\tvalidation_0-logloss:0.45355\tvalidation_0-auc:0.90710\tvalidation_1-logloss:0.61906\tvalidation_1-auc:0.64157\n",
      "[36]\tvalidation_0-logloss:0.44960\tvalidation_0-auc:0.90909\tvalidation_1-logloss:0.61682\tvalidation_1-auc:0.64545\n",
      "[37]\tvalidation_0-logloss:0.44468\tvalidation_0-auc:0.91403\tvalidation_1-logloss:0.61553\tvalidation_1-auc:0.64559\n",
      "[38]\tvalidation_0-logloss:0.44303\tvalidation_0-auc:0.91492\tvalidation_1-logloss:0.61536\tvalidation_1-auc:0.64572\n",
      "[39]\tvalidation_0-logloss:0.43908\tvalidation_0-auc:0.91787\tvalidation_1-logloss:0.61403\tvalidation_1-auc:0.64558\n",
      "[40]\tvalidation_0-logloss:0.43663\tvalidation_0-auc:0.91963\tvalidation_1-logloss:0.61303\tvalidation_1-auc:0.64697\n",
      "[41]\tvalidation_0-logloss:0.43200\tvalidation_0-auc:0.92407\tvalidation_1-logloss:0.61190\tvalidation_1-auc:0.64735\n",
      "[42]\tvalidation_0-logloss:0.42745\tvalidation_0-auc:0.92668\tvalidation_1-logloss:0.61149\tvalidation_1-auc:0.64793\n",
      "[43]\tvalidation_0-logloss:0.42201\tvalidation_0-auc:0.93058\tvalidation_1-logloss:0.61005\tvalidation_1-auc:0.64781\n",
      "[44]\tvalidation_0-logloss:0.41828\tvalidation_0-auc:0.93274\tvalidation_1-logloss:0.60927\tvalidation_1-auc:0.64845\n",
      "[45]\tvalidation_0-logloss:0.41781\tvalidation_0-auc:0.93290\tvalidation_1-logloss:0.60954\tvalidation_1-auc:0.64817\n",
      "[46]\tvalidation_0-logloss:0.41216\tvalidation_0-auc:0.93775\tvalidation_1-logloss:0.60753\tvalidation_1-auc:0.64784\n",
      "[47]\tvalidation_0-logloss:0.40766\tvalidation_0-auc:0.93999\tvalidation_1-logloss:0.60678\tvalidation_1-auc:0.64813\n",
      "[48]\tvalidation_0-logloss:0.40615\tvalidation_0-auc:0.94068\tvalidation_1-logloss:0.60736\tvalidation_1-auc:0.64694\n",
      "[49]\tvalidation_0-logloss:0.39779\tvalidation_0-auc:0.94546\tvalidation_1-logloss:0.60560\tvalidation_1-auc:0.64676\n",
      "[50]\tvalidation_0-logloss:0.39326\tvalidation_0-auc:0.94771\tvalidation_1-logloss:0.60486\tvalidation_1-auc:0.64665\n",
      "[51]\tvalidation_0-logloss:0.39061\tvalidation_0-auc:0.94876\tvalidation_1-logloss:0.60580\tvalidation_1-auc:0.64561\n",
      "[52]\tvalidation_0-logloss:0.38984\tvalidation_0-auc:0.94909\tvalidation_1-logloss:0.60494\tvalidation_1-auc:0.64634\n",
      "[53]\tvalidation_0-logloss:0.38550\tvalidation_0-auc:0.95082\tvalidation_1-logloss:0.60412\tvalidation_1-auc:0.64745\n",
      "[54]\tvalidation_0-logloss:0.38172\tvalidation_0-auc:0.95331\tvalidation_1-logloss:0.60449\tvalidation_1-auc:0.64549\n",
      "[55]\tvalidation_0-logloss:0.38141\tvalidation_0-auc:0.95338\tvalidation_1-logloss:0.60466\tvalidation_1-auc:0.64554\n",
      "[56]\tvalidation_0-logloss:0.38025\tvalidation_0-auc:0.95379\tvalidation_1-logloss:0.60477\tvalidation_1-auc:0.64504\n",
      "[57]\tvalidation_0-logloss:0.37727\tvalidation_0-auc:0.95449\tvalidation_1-logloss:0.60451\tvalidation_1-auc:0.64508\n",
      "[58]\tvalidation_0-logloss:0.37676\tvalidation_0-auc:0.95464\tvalidation_1-logloss:0.60454\tvalidation_1-auc:0.64507\n",
      "[59]\tvalidation_0-logloss:0.37373\tvalidation_0-auc:0.95592\tvalidation_1-logloss:0.60386\tvalidation_1-auc:0.64575\n",
      "[60]\tvalidation_0-logloss:0.37040\tvalidation_0-auc:0.95698\tvalidation_1-logloss:0.60314\tvalidation_1-auc:0.64623\n",
      "[61]\tvalidation_0-logloss:0.37004\tvalidation_0-auc:0.95709\tvalidation_1-logloss:0.60271\tvalidation_1-auc:0.64649\n",
      "[62]\tvalidation_0-logloss:0.36971\tvalidation_0-auc:0.95719\tvalidation_1-logloss:0.60297\tvalidation_1-auc:0.64634\n",
      "[63]\tvalidation_0-logloss:0.36578\tvalidation_0-auc:0.95923\tvalidation_1-logloss:0.60370\tvalidation_1-auc:0.64613\n",
      "[64]\tvalidation_0-logloss:0.36063\tvalidation_0-auc:0.96216\tvalidation_1-logloss:0.60281\tvalidation_1-auc:0.64669\n",
      "[65]\tvalidation_0-logloss:0.36022\tvalidation_0-auc:0.96234\tvalidation_1-logloss:0.60209\tvalidation_1-auc:0.64710\n",
      "[66]\tvalidation_0-logloss:0.35996\tvalidation_0-auc:0.96242\tvalidation_1-logloss:0.60219\tvalidation_1-auc:0.64702\n",
      "[67]\tvalidation_0-logloss:0.35706\tvalidation_0-auc:0.96369\tvalidation_1-logloss:0.60220\tvalidation_1-auc:0.64695\n",
      "[68]\tvalidation_0-logloss:0.35033\tvalidation_0-auc:0.96617\tvalidation_1-logloss:0.60123\tvalidation_1-auc:0.64728\n",
      "[69]\tvalidation_0-logloss:0.35000\tvalidation_0-auc:0.96626\tvalidation_1-logloss:0.60119\tvalidation_1-auc:0.64725\n",
      "[70]\tvalidation_0-logloss:0.34855\tvalidation_0-auc:0.96680\tvalidation_1-logloss:0.60199\tvalidation_1-auc:0.64666\n",
      "[71]\tvalidation_0-logloss:0.34646\tvalidation_0-auc:0.96755\tvalidation_1-logloss:0.60262\tvalidation_1-auc:0.64606\n",
      "[72]\tvalidation_0-logloss:0.34607\tvalidation_0-auc:0.96763\tvalidation_1-logloss:0.60260\tvalidation_1-auc:0.64597\n",
      "[73]\tvalidation_0-logloss:0.34547\tvalidation_0-auc:0.96784\tvalidation_1-logloss:0.60239\tvalidation_1-auc:0.64601\n",
      "[74]\tvalidation_0-logloss:0.34417\tvalidation_0-auc:0.96811\tvalidation_1-logloss:0.60269\tvalidation_1-auc:0.64587\n",
      "[75]\tvalidation_0-logloss:0.33931\tvalidation_0-auc:0.97026\tvalidation_1-logloss:0.60276\tvalidation_1-auc:0.64482\n",
      "[76]\tvalidation_0-logloss:0.33857\tvalidation_0-auc:0.97040\tvalidation_1-logloss:0.60258\tvalidation_1-auc:0.64501\n",
      "[77]\tvalidation_0-logloss:0.33534\tvalidation_0-auc:0.97161\tvalidation_1-logloss:0.60273\tvalidation_1-auc:0.64502\n",
      "[78]\tvalidation_0-logloss:0.33462\tvalidation_0-auc:0.97174\tvalidation_1-logloss:0.60272\tvalidation_1-auc:0.64492\n",
      "[79]\tvalidation_0-logloss:0.32996\tvalidation_0-auc:0.97338\tvalidation_1-logloss:0.60304\tvalidation_1-auc:0.64248\n",
      "[80]\tvalidation_0-logloss:0.32675\tvalidation_0-auc:0.97443\tvalidation_1-logloss:0.60305\tvalidation_1-auc:0.64263\n",
      "[81]\tvalidation_0-logloss:0.32297\tvalidation_0-auc:0.97540\tvalidation_1-logloss:0.60172\tvalidation_1-auc:0.64377\n",
      "[82]\tvalidation_0-logloss:0.32178\tvalidation_0-auc:0.97568\tvalidation_1-logloss:0.60186\tvalidation_1-auc:0.64286\n",
      "[83]\tvalidation_0-logloss:0.31959\tvalidation_0-auc:0.97634\tvalidation_1-logloss:0.60121\tvalidation_1-auc:0.64307\n",
      "[84]\tvalidation_0-logloss:0.31780\tvalidation_0-auc:0.97692\tvalidation_1-logloss:0.60181\tvalidation_1-auc:0.64170\n",
      "[85]\tvalidation_0-logloss:0.31717\tvalidation_0-auc:0.97706\tvalidation_1-logloss:0.60184\tvalidation_1-auc:0.64149\n",
      "[86]\tvalidation_0-logloss:0.31240\tvalidation_0-auc:0.97836\tvalidation_1-logloss:0.59870\tvalidation_1-auc:0.64520\n",
      "[87]\tvalidation_0-logloss:0.30996\tvalidation_0-auc:0.97908\tvalidation_1-logloss:0.59858\tvalidation_1-auc:0.64489\n",
      "[88]\tvalidation_0-logloss:0.30822\tvalidation_0-auc:0.97940\tvalidation_1-logloss:0.59877\tvalidation_1-auc:0.64505\n",
      "[89]\tvalidation_0-logloss:0.30751\tvalidation_0-auc:0.97954\tvalidation_1-logloss:0.59901\tvalidation_1-auc:0.64533\n",
      "[90]\tvalidation_0-logloss:0.30657\tvalidation_0-auc:0.97983\tvalidation_1-logloss:0.59868\tvalidation_1-auc:0.64508\n",
      "[91]\tvalidation_0-logloss:0.30437\tvalidation_0-auc:0.98016\tvalidation_1-logloss:0.59883\tvalidation_1-auc:0.64532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92]\tvalidation_0-logloss:0.30282\tvalidation_0-auc:0.98047\tvalidation_1-logloss:0.59861\tvalidation_1-auc:0.64489\n",
      "[93]\tvalidation_0-logloss:0.30217\tvalidation_0-auc:0.98055\tvalidation_1-logloss:0.59977\tvalidation_1-auc:0.64376\n",
      "[94]\tvalidation_0-logloss:0.29968\tvalidation_0-auc:0.98122\tvalidation_1-logloss:0.59897\tvalidation_1-auc:0.64352\n",
      "[95]\tvalidation_0-logloss:0.29834\tvalidation_0-auc:0.98163\tvalidation_1-logloss:0.59880\tvalidation_1-auc:0.64324\n",
      "[96]\tvalidation_0-logloss:0.29628\tvalidation_0-auc:0.98209\tvalidation_1-logloss:0.59846\tvalidation_1-auc:0.64398\n",
      "[97]\tvalidation_0-logloss:0.29557\tvalidation_0-auc:0.98225\tvalidation_1-logloss:0.59837\tvalidation_1-auc:0.64418\n",
      "[98]\tvalidation_0-logloss:0.29197\tvalidation_0-auc:0.98299\tvalidation_1-logloss:0.59787\tvalidation_1-auc:0.64461\n",
      "[99]\tvalidation_0-logloss:0.29064\tvalidation_0-auc:0.98318\tvalidation_1-logloss:0.59804\tvalidation_1-auc:0.64427\n"
     ]
    }
   ],
   "source": [
    "xgb_baseline         = xgb.XGBClassifier(random_state = seed, n_jobs=cpu_count()//2) #seed)\n",
    "eval_set             = [(X_train_upsampled.values, Y_train_upsampled.values), (X_test.values, Y_test.values)]\n",
    "trained_xgb_baseline = xgb_baseline.fit(X_train_upsampled.values, Y_train_upsampled.values\\\n",
    "                                        , eval_metric = eval_metric \\\n",
    "                                        , eval_set = eval_set, verbose=True)\n",
    "trained_xgb_baseline = xgb_baseline.fit(X_train_upsampled, Y_train_upsampled, eval_metric = eval_metric)\n",
    "feature_importances  = xgb_baseline.feature_importances_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"model_artifacts/xgb_baseline.pkl\"\n",
    "\n",
    "# save\n",
    "pickle.dump(trained_xgb_baseline, open(file_name, \"wb\"))\n",
    "\n",
    "# load\n",
    "trained_xgb_baseline = pickle.load(open(file_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Checking overfitting/underfitting & on number of trees using rmse and mae\n",
    "This should check should truly be done using a test/val dataset, hence in this case is used for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trained_xgb_baseline.evals_result()\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('XGBoost Baseline - logloss')\n",
    "plt.show()\n",
    "# plot classification error\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('auc')\n",
    "plt.title('XGBoost Baseline - auc ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Baseline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_func(func, y_data, predictions):\n",
    "    try:\n",
    "        return func(y_data,predictions).round(2)\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        return np.nan\n",
    "\n",
    "def calc_metrics(classifier, x_data, y_data):\n",
    "\n",
    "    shape_x, shape_y = (x_data.shape, y_data.shape)\n",
    "    print('Data shapes X:', shape_x, 'Y:', shape_y)\n",
    "    predictions = classifier.predict(x_data) \n",
    "\n",
    "    \n",
    "    acc  = try_func(accuracy_score, y_data, predictions)\n",
    "    f1   = try_func(f1_score, y_data, predictions)\n",
    "    prec = try_func(precision_score, y_data, predictions)\n",
    "    rec  = try_func(recall_score, y_data, predictions)\n",
    "    print('Metrics model -', 'accuracy:', acc, 'f1:', f1, 'precision:', prec, 'recall:', rec) \n",
    "    roc  = try_func(roc_auc_score, y_data, predictions)\n",
    "    \n",
    "\n",
    "    print('Metrics model -', 'accuracy:', acc, 'f1:', f1, 'precision:', prec, 'recall:', rec, 'roc_auc:', roc) \n",
    "    return [acc, f1, prec, rec, roc, shape_x, shape_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics XGB classifier baseline on train data\n",
      "Data shapes X: (12028, 16) Y: (12028, 1)\n",
      "Metrics model - accuracy: 0.93 f1: 0.93 precision: 0.9 recall: 0.97\n",
      "Metrics model - accuracy: 0.93 f1: 0.93 precision: 0.9 recall: 0.97 roc_auc: 0.93\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics XGB classifier baseline on train data\")\n",
    "results_baseline_train = calc_metrics(trained_xgb_baseline, X_train_upsampled.values, Y_train_upsampled.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics XGB classifier baseline on test data\n",
      "Data shapes X: (1943, 16) Y: (1943, 1)\n",
      "Metrics model - accuracy: 0.69 f1: 0.39 precision: 0.35 recall: 0.44\n",
      "Metrics model - accuracy: 0.69 f1: 0.39 precision: 0.35 recall: 0.44 roc_auc: 0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics XGB classifier baseline on test data\")\n",
    "results_baseline_test = calc_metrics(trained_xgb_baseline, X_test.values, Y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Fine tuning the XGB model (takes about 4 mins in 6 core laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I take inspiration from here https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_Options = { \n",
    "                'n_jobs':           1, #cpu_count()//2,\n",
    "                'cv':               3,\n",
    "                'scoring':          'roc_auc',#'roc_auc',\n",
    "                'seed':             seed, \n",
    "    \n",
    "                'max_depth':        np.arange(2,12,4),\n",
    "                'min_child_weight': np.arange(1,10,3),\n",
    "                'n_estimators':     [50, 100, 200], \n",
    "\n",
    "                'gamma':            np.arange(0.05,0.45,0.15),\n",
    "                'colsample_bytree': np.arange(0.60, 0.95, 0.15),\n",
    "                'subsample':        np.arange(0.60, 0.95, 0.15),\n",
    "                'reg_alpha':        [1e-6, 1, 10], \n",
    "                'reg_lambda':       [1e-6, 1, 10],\n",
    "                'learning_rate':    np.arange(0.025,0.150,0.050),\n",
    "                 'verbose':         1            \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with low learning rate and tuning: max_depth, min_child_weight, n_estimators\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Time to fit 0:00:43.454376\n",
      "best_params_: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "best_score_: 0.9371301755044894\n",
      "Tuning: gamma\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Time to fit xgb 0:00:15.990208\n",
      "best_params_: {'colsample_bytree': 0.8, 'gamma': 0.05, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "best_score_: 0.9371465370409876\n",
      "Tuning: colsample_bytree, subsample\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Time to fit 0:00:47.187688\n",
      "best_params_: {'colsample_bytree': 0.9, 'gamma': 0.05, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "best_score_: 0.9379018259027371\n",
      "Tuning: reg_alpha, reg_lambda\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "#xgb_reg = xgb.XGBRegressor(random_state=XGB_Options['seed'], n_jobs=cpu_count()//2) \n",
    "xgb_reg = xgb.XGBClassifier(random_state = XGB_Options['seed']\\\n",
    "                            , n_jobs = cpu_count()//2\\\n",
    "                            , eval_metric = eval_metric, use_label_encoder=False) \n",
    "start = datetime.datetime.now()\n",
    "print('Starting with low learning rate and tuning: max_depth, min_child_weight, n_estimators')\n",
    "\n",
    "params = {  \n",
    "    \"learning_rate\":     [0.1],\n",
    "    \"max_depth\":         XGB_Options['max_depth'], \n",
    "    \"min_child_weight\":  XGB_Options['min_child_weight'], \n",
    "    \"n_estimators\":      XGB_Options['n_estimators'], \n",
    "\n",
    "    \"colsample_bytree\":  [0.8], \n",
    "    \"subsample\":         [0.8],\n",
    "    \"gamma\":             [0],\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(xgb_reg, \n",
    "                    params,\n",
    "                    cv                 = XGB_Options['cv'],\n",
    "                    scoring            = XGB_Options['scoring'], \n",
    "                    n_jobs             = XGB_Options['n_jobs'], \n",
    "                    verbose            = XGB_Options['verbose'], \n",
    "                    return_train_score = True)\n",
    "\n",
    "GSCV.fit(X_train_upsampled.values, Y_train_upsampled.values)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print('Time to fit', (end-start))\n",
    "print('best_params_:', GSCV.best_params_)#, \n",
    "print('best_score_:',  GSCV.best_score_)\n",
    "\n",
    "print('Tuning: gamma')\n",
    "start = datetime.datetime.now()\n",
    "params = {  \n",
    "    \"learning_rate\":    [0.1], \n",
    "    \"max_depth\":        [GSCV.best_params_['max_depth']],\n",
    "    \"min_child_weight\": [GSCV.best_params_['min_child_weight']],\n",
    "    \"n_estimators\":     [GSCV.best_params_['n_estimators']],\n",
    "\n",
    "    \"colsample_bytree\": [0.8], \n",
    "    \"subsample\":        [0.8],\n",
    "    \"gamma\":            XGB_Options['gamma'],\n",
    "\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(xgb_reg, \n",
    "                    params,\n",
    "                    cv                 = XGB_Options['cv'],\n",
    "                    scoring            = XGB_Options['scoring'], \n",
    "                    n_jobs             = XGB_Options['n_jobs'],\n",
    "                    verbose            = XGB_Options['verbose'], \n",
    "                    return_train_score = True)\n",
    "\n",
    "GSCV.fit(X_train_upsampled.values, Y_train_upsampled.values)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print('Time to fit xgb', (end-start))\n",
    "print('best_params_:', GSCV.best_params_)#, \n",
    "print('best_score_:', GSCV.best_score_)\n",
    "\n",
    "\n",
    "print('Tuning: colsample_bytree, subsample')\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "params = {  \n",
    "    \"learning_rate\":    [0.1], \n",
    "    \"max_depth\":        [GSCV.best_params_['max_depth']],\n",
    "    \"min_child_weight\": [GSCV.best_params_['min_child_weight']],\n",
    "    \"n_estimators\":     [GSCV.best_params_['n_estimators']],\n",
    "    \"gamma\":            [GSCV.best_params_['gamma']],\n",
    "\n",
    "    \"colsample_bytree\": XGB_Options['colsample_bytree'],\n",
    "    \"subsample\":        XGB_Options['subsample'],\n",
    "\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(xgb_reg, \n",
    "                    params,\n",
    "                    cv                 = XGB_Options['cv'],\n",
    "                    scoring            = XGB_Options['scoring'], \n",
    "                    n_jobs             = XGB_Options['n_jobs'],\n",
    "                    verbose            = XGB_Options['verbose'], \n",
    "                    return_train_score = True)\n",
    "\n",
    "GSCV.fit(X_train_upsampled.values, Y_train_upsampled.values)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print('Time to fit', (end-start))\n",
    "print('best_params_:', GSCV.best_params_) \n",
    "print('best_score_:', GSCV.best_score_)\n",
    "\n",
    "print('Tuning: reg_alpha, reg_lambda')\n",
    "start = datetime.datetime.now()\n",
    "params = {  \n",
    "    \"learning_rate\":    [0.1], \n",
    "    \"max_depth\":        [GSCV.best_params_['max_depth']],\n",
    "    \"min_child_weight\": [GSCV.best_params_['min_child_weight']],\n",
    "    \"n_estimators\":     [GSCV.best_params_['n_estimators']],\n",
    "    \"gamma\":            [GSCV.best_params_['gamma']],\n",
    "\n",
    "    \"colsample_bytree\": [GSCV.best_params_['colsample_bytree']], \n",
    "    \"subsample\":        [GSCV.best_params_['subsample']],\n",
    "\n",
    "\n",
    "    \"reg_alpha\":        XGB_Options['reg_alpha'], \n",
    "    \"reg_lambda\":       XGB_Options['reg_lambda'], \n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(xgb_reg, \n",
    "                    params,\n",
    "                    cv                 = XGB_Options['cv'],\n",
    "                    scoring            = XGB_Options['scoring'], \n",
    "                    n_jobs             = XGB_Options['n_jobs'],\n",
    "                    verbose            = XGB_Options['verbose'], \n",
    "                    return_train_score = True)\n",
    "\n",
    "GSCV.fit(X_train_upsampled.values, Y_train_upsampled.values)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print('Time to fit', (end-start))\n",
    "print('best_params_:', GSCV.best_params_)\n",
    "print('best_score_:', GSCV.best_score_)\n",
    "\n",
    "\n",
    "print('Tuning: learning_rate')\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "params = {  \n",
    "    \"learning_rate\":    XGB_Options['learning_rate'], \n",
    "    \"max_depth\":        [GSCV.best_params_['max_depth']],\n",
    "    \"min_child_weight\": [GSCV.best_params_['min_child_weight']],\n",
    "    \"n_estimators\":     [GSCV.best_params_['n_estimators']],\n",
    "    \"gamma\":            [GSCV.best_params_['gamma']],\n",
    "\n",
    "    \"colsample_bytree\": [GSCV.best_params_['colsample_bytree']], \n",
    "    \"subsample\":        [GSCV.best_params_['subsample']],\n",
    "\n",
    "\n",
    "    \"reg_alpha\":        [GSCV.best_params_['reg_alpha']],\n",
    "    \"reg_lambda\":       [GSCV.best_params_['reg_lambda']]\n",
    "}\n",
    "\n",
    "GSCV = GridSearchCV(xgb_reg, \n",
    "                    params,\n",
    "                    cv                 = XGB_Options['cv'],\n",
    "                    scoring            = XGB_Options['scoring'], \n",
    "                    n_jobs             = XGB_Options['n_jobs'],\n",
    "                    verbose            = XGB_Options['verbose'], \n",
    "                    return_train_score = True)\n",
    "\n",
    "GSCV.fit(X_train_upsampled.values, Y_train_upsampled.values) \n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print('Time to fit', (end-start))\n",
    "print('best_params_:', GSCV.best_params_)\n",
    "print('best_score_:', GSCV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "start = datetime.datetime.now()\n",
    "xgb_tuned           = xgb.XGBClassifier(random_state=XGB_Options['seed']\\\n",
    "                                        , n_jobs=cpu_count()//2\\\n",
    "                                        , use_label_encoder=False) #seed)\n",
    "xgb_tuned.set_params(**GSCV.best_params_)\n",
    "trained_xgb_tuned   = xgb_tuned.fit(X_train_upsampled.values, Y_train_upsampled.values\\\n",
    "                                    , eval_metric = eval_metric \\\n",
    "                                    , eval_set = eval_set, verbose=False)\n",
    "feature_importances = trained_xgb_tuned.feature_importances_ \n",
    "end = datetime.datetime.now()\n",
    "print('Time to fit xgb', (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Tuned Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"model_artifacts/xgb_tuned.pkl\"\n",
    "\n",
    "# save\n",
    "pickle.dump(trained_xgb_tuned, open(file_name, \"wb\"))\n",
    "\n",
    "# load\n",
    "trained_xgb_tuned = pickle.load(open(file_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics XGB classifier tuned on train data\")\n",
    "metrics_tuned_model_train = calc_metrics(trained_xgb_tuned, X_train_upsampled.values, Y_train_upsampled.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics XGB classifier tuned on test data\")\n",
    "metrics_tuned_model_test =  calc_metrics(trained_xgb_tuned, X_test.values, Y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Comparing Metrics with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the results in 2.1.2 the 'tuned' model performs better than the baseline model. Though this seems to come from a more complex model, as it's a lot deeper and with more boosting operations than the baseline. An issue that this could cause is overfitting, i.e. if this model was deployed to production it's performance might not be as good as in this offline setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = ['accuracy_score', 'f1_score', 'precision_score', 'recall_score', 'roc_auc_score'\\\n",
    "                 , 'shape_x', 'shape_y']\n",
    "columns = ['subset', 'sex', 'education', 'marriage' ] + metrics_names\n",
    "metrics = pd.DataFrame([ ['Train'] + ['All'] + ['All'] + ['All'] + metrics_tuned_model_train\\\n",
    "                         , ['Test'] + ['All'] + ['All'] + ['All'] + metrics_tuned_model_test]\\\n",
    "                       , columns = columns)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Model Metrics per category In/Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_categories = pd.read_csv('data_artifacts/combination_categories.csv')\n",
    "combination_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_datasets = [['Train', X_train_upsampled, Y_train_upsampled], ['Test', X_test, Y_test]]\n",
    "\n",
    "for subset, dataset_x, dataset_y in metric_datasets:\n",
    "    for i, row in combination_categories.iterrows():\n",
    "\n",
    "        x_test_cat = dataset_x[ (dataset_x[row.SEX] == 1) \\\n",
    "                               & (dataset_x[row.EDUCATION] == 1)\\\n",
    "                               & (dataset_x[row.MARRIAGE] == 1)]\n",
    "        y_test_cat = dataset_y.loc[x_test_cat.index, :]\n",
    "\n",
    "        print('subset:', subset,'SEX:', row.SEX, 'EDUCATION:', row.EDUCATION, 'MARRIAGE:', row.MARRIAGE)\n",
    "        \"\"\"\n",
    "        try: \n",
    "            results_cat = calc_metrics(trained_xgb_tuned, x_test_cat.values, y_test_cat.values)\n",
    "        except Exception as E:\n",
    "            print(E)\n",
    "            pass\n",
    "        \"\"\"\n",
    "        results_cat = calc_metrics(trained_xgb_tuned, x_test_cat.values, y_test_cat.values)        \n",
    "        aux_df      = pd.DataFrame([ [subset] + [row.SEX] + [row.EDUCATION] + [row.MARRIAGE] +  results_cat]\\\n",
    "                               , columns = columns)    \n",
    "    \n",
    "        metrics     = metrics.append(aux_df)\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[ metrics.roc_auc_score.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[(metrics.sex == 'All') & (metrics.education == 'All') & (metrics.marriage == 'All') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Metrics by categories  - visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = metrics[ (metrics['sex'] =='All') & (metrics['education'] =='All') & (metrics['marriage'] =='All')]\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plot_metric = \"roc_auc_score\"\n",
    "ax          = sns.barplot(x = \"subset\", y = plot_metric, data = plot_df)\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize = 20)\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize = 20);\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize = 20);\n",
    "\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(20)\n",
    "\n",
    "    \n",
    "ax.set_ylim(0, 1.07);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = metrics[ (metrics['sex'] !='All') & (metrics['subset'] == 'Train')].copy()\n",
    "plot_df['category'] = plot_df['subset'] + '-' + plot_df['sex']\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plot_metric = \"roc_auc_score\"\n",
    "ax = sns.barplot(x=\"education\", y = plot_metric, hue = 'category', data= plot_df)\n",
    "#ax = sns.barplot(x = \"education\", y = plot_metric, hue = 'sex', data= plot_df)\n",
    "\n",
    "plt.xticks(rotation=-45, ha='left');    \n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize = 14, fmt='%.2f')\n",
    "    \n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize = 20);\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize = 20);\n",
    "\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    #label.set_fontname('Arial')\n",
    "    label.set_fontsize(20)\n",
    "\n",
    "#plt.legend(fontsize = 20)\n",
    "#plt.setp( plt.gca().get_legend().get_title(),fontsize= 20);\n",
    "ax.set_ylim(0, 1.07)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.0), framealpha = 1, fontsize = 14, \n",
    "          title_fontsize = 20,title=ax.legend_.get_title().get_text());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = metrics[ (metrics['sex'] !='All') & (metrics['subset'] == 'Test')].copy()\n",
    "plot_df['category'] = plot_df['subset'] + '-' + plot_df['sex']\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plot_metric = \"roc_auc_score\"\n",
    "ax = sns.barplot(x=\"education\", y = plot_metric, hue = 'category', data= plot_df)\n",
    "#ax = sns.barplot(x = \"education\", y = plot_metric, hue = 'sex', data= plot_df)\n",
    "\n",
    "plt.xticks(rotation=-45, ha='left');    \n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize = 14, fmt='%.2f')\n",
    "    \n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize = 20);\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize = 20);\n",
    "\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    #label.set_fontname('Arial')\n",
    "    label.set_fontsize(20)\n",
    "\n",
    "#plt.legend(fontsize = 20)\n",
    "#plt.setp( plt.gca().get_legend().get_title(),fontsize= 20);\n",
    "ax.set_ylim(0, 1.07)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.0), framealpha = 1, fontsize = 14, \n",
    "          title_fontsize = 20,title=ax.legend_.get_title().get_text());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = metrics[ (metrics['marriage'] !='All') & (metrics['subset'] == 'Train')].copy()\n",
    "plot_df['category'] = plot_df['subset'] + '-' + plot_df['marriage']\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plot_metric = \"roc_auc_score\"\n",
    "ax = sns.barplot(x=\"education\", y = plot_metric, hue = 'category', data= plot_df)\n",
    "#ax = sns.barplot(x = \"education\", y = plot_metric, hue = 'sex', data= plot_df)\n",
    "\n",
    "plt.xticks(rotation=-45, ha='left');    \n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize = 14, fmt='%.2f')\n",
    "    \n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize = 20);\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize = 20);\n",
    "\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    #label.set_fontname('Arial')\n",
    "    label.set_fontsize(20)\n",
    "\n",
    "#plt.legend(fontsize = 20)\n",
    "#plt.setp( plt.gca().get_legend().get_title(),fontsize= 20);\n",
    "ax.set_ylim(0, 1.07)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.0), framealpha = 1, fontsize = 14, \n",
    "          title_fontsize = 20,title=ax.legend_.get_title().get_text());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = metrics[ (metrics['marriage'] !='All') & (metrics['subset'] == 'Test')].copy()\n",
    "plot_df['category'] = plot_df['subset'] + '-' + plot_df['marriage']\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plot_metric = \"roc_auc_score\"\n",
    "ax = sns.barplot(x=\"education\", y = plot_metric, hue = 'category', data= plot_df)\n",
    "#ax = sns.barplot(x = \"education\", y = plot_metric, hue = 'sex', data= plot_df)\n",
    "\n",
    "plt.xticks(rotation=-45, ha='left');    \n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fontsize = 14, fmt='%.2f')\n",
    "    \n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize = 20);\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize = 20);\n",
    "\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    #label.set_fontname('Arial')\n",
    "    label.set_fontsize(20)\n",
    "\n",
    "#plt.legend(fontsize = 20)\n",
    "#plt.setp( plt.gca().get_legend().get_title(),fontsize= 20);\n",
    "ax.set_ylim(0, 1.07)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.0), framealpha = 1, fontsize = 14, \n",
    "          title_fontsize = 20,title=ax.legend_.get_title().get_text());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0  Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Weight, Gain, Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Available importance_types = [weight, gain, cover, total_gain, total_cover]\n",
    "\n",
    "pal = sns.color_palette('tab10')\n",
    "\n",
    "feature_names = X_train_upsampled.columns\n",
    "\n",
    "for importance in ['weight', 'gain', 'cover', 'total_gain', 'total_cover']:\n",
    "    dict_imp = trained_xgb_tuned.get_booster().get_score(importance_type= importance)\n",
    "    \n",
    "    imp_df = pd.DataFrame.from_dict(dict_imp, orient='index', columns = ['values'])\n",
    "    imp_df = imp_df.reset_index().sort_values(by='values', ascending = False)\n",
    "    \n",
    "    plt.figure(figsize = (10,8))\n",
    "    ax = sns.barplot(x=\"values\", y=\"index\", data=imp_df, color = pal.as_hex()[0])\n",
    "    plt.xticks(fontsize=14, rotation=0)\n",
    "    \n",
    "    y_ticks = []\n",
    "    for item in ax.get_yticklabels():\n",
    "        index_feature = int(item.get_text().replace('f', ''))\n",
    "        y_ticks.append(feature_names[index_feature])\n",
    "       \n",
    "    ax.set_yticklabels(y_ticks)\n",
    "    plt.yticks(fontsize=14, rotation=0)\n",
    "    ax.set_xlabel(\"Feature importance using: \" + importance, fontsize =20)\n",
    "    ax.set_ylabel(\"Feature\" + importance, fontsize =20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Shap values on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(trained_xgb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_train_upsampled)\n",
    "#shap_values = explainer(X_train, X100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_artifacts/shap_values_train.pkl\",\"wb\") as f:\n",
    "    pickle.dump(shap_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_artifacts/shap_values_train.pkl\",\"rb\") as f:\n",
    "    shap_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_train_upsampled.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_upsampled, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact of each feature in a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Shap values on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(trained_xgb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_test)\n",
    "#shap_values = explainer(X_train_upsampled, X100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_artifacts/shap_values_test.pkl\",\"wb\") as f:\n",
    "    pickle.dump(shap_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_artifacts/shap_values_test.pkl\",\"rb\") as f:\n",
    "    shap_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact of each feature in a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Avenues of improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and feature improvements:\n",
    "- The categories with small number of records, could be merged into one, e.g. unknown and others\n",
    "\n",
    "In terms of model training and evaluation:\n",
    "- It seems that the dataset is overfitting the train dataset, thus having not so great result in the test, thefore this could be an area to improvement, perhaps creating including more features that allow the model to learn the dataset from another perspective (include the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
